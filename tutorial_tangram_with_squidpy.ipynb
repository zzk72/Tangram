{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for spatial mapping using _Tangram_\n",
    "- by Ziqing Lu <luz21@gene.com> and Tommaso Biancalani <biancalt@gene.com>.\n",
    "- Last update: August 16th 2021\n",
    "\n",
    "### What is _Tangram_?\n",
    "_Tangram_ is a method for mapping single-cell (or single-nucleus) gene expression data onto spatial gene expression data. _Tangram_ takes as input a single-cell dataset and a spatial dataset, collected from the same anatomical region/tissue type. Via integration,  _Tangram_ <u>creates new spatial data by aligning the scRNAseq profiles in space</u>. This allows to project every annotation in the scRNAseq (_e.g._ cell types, program usage) on space.\n",
    "\n",
    "\n",
    "### What do I use _Tangram_ for?\n",
    "The most common application of _Tangram_ is to resolve cell types in space. Another usage is to correct gene expression from spatial data: as scRNA-seq data are less prone to dropout than (_e.g._) Visium or Slide-seq, the \"new\" spatial data generated by _Tangram_ resolve many more genes. As a result, we can visualize program usage in space, which can be used for ligand-receptor pair discovery or, more generally, cell-cell communication mechanisms. If cell segmentation is available, _Tangram_ can be also used for deconvolution of spatial data. If your single cell are multimodal, _Tangram_ can be used to spatially resolve other modalities, such as chromatin accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Frequently Asked Questions about _Tangram_\n",
    "\n",
    "##### How is _Tangram_ different, than all the other deconvolution/mapping method?\n",
    "- Validation. Most methods \"validate\" mappings by looking at known patterns or proportion of cell types. These are good sanity checks, but are hardly useful when mapping is used for discovery. In _Tangram_, mappings are validated by inspective the predictions of holdout genes (test transcriptome).\n",
    "\n",
    "\n",
    "##### My scRNAseq/spatial data come from different samples. Can I still use _Tangram_?\n",
    "- Yes. There is a clever variation invented by [Sten Linnarsson](http://linnarssonlab.org/), which consists of mapping _average_ cells of a certain cell type, rather than single cells. This method is much faster, and  smooths out variation in biological signal from different samples via averaging. However, it requires annotated scRNA-seq, sacrifices resolving biological variability at single-cell level. To map this way, pass `mode=cluster`.\n",
    "\n",
    "\n",
    "##### Does _Tangram_ only work on mouse brain data?\n",
    "- **No.** The original manuscript focused on mouse brain data b/c was funded by BICCN. We subsequently used _Tangram_ for mapping lung, kidney and cancer tissue. If mapping doesn't work for your case, that is hardly due to the complexity of the tissue.\n",
    "\n",
    "\n",
    "##### Why doesn't _Tangram_ have hypotheses on the underlying model?\n",
    "- Most models used in biology are probabilistic: they assume that data are generated according to a certain probability distribution, hence the hypothesis. But _Tangram_ doesn't work that way: the hypothesis is that scRNA-seq and spatial data are generated with the same process (_i.e._ same biology) regardless of the process.\n",
    "\n",
    "\n",
    "##### Where do I learn more about _Tangram_?\n",
    "- Check out our [documentation](https://tangram-sc.readthedocs.io/) for learning more about the method, or our [GitHub repo](https://github.com/broadinstitute/Tangram) for the latest version of the code. Tangram has been released in <cite data-cite=\"tangram\">Biancalani, Scalia et al. Nature Methods (2021)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Setting up\n",
    "_Tangram_ is based on pytorch, scanpy and (optionally but highly-recommended) squidpy - this tutorial is designed to work with squidy.  You can also check [this tutorial](https://github.com/broadinstitute/Tangram/blob/master/tangram_tutorial.ipynb), prior to integration with squidpy. \n",
    "\n",
    "- To run the notebook locally, create a conda environment as `conda env create -f tangram_environment.yml` using our [YAML file](https://github.com/theislab/squidpy_notebooks/blob/master/envs/tangram_environment.yml). \n",
    "- This notebook is based on squidpy v1.1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:16:57.369370100Z",
     "start_time": "2023-09-09T08:16:52.655679Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mseaborn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msns\u001B[39;00m\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtangram\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtg\u001B[39;00m\n\u001B[0;32m     15\u001B[0m sc\u001B[38;5;241m.\u001B[39mlogging\u001B[38;5;241m.\u001B[39mprint_header()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msquidpy==\u001B[39m\u001B[38;5;132;01m{\u001B[39;00msq\u001B[38;5;241m.\u001B[39m__version__\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\CODE_DATA\\Scientific_Research\\Tangram-master\\tangram\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_version\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m __version__\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mplot_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[1;32mD:\\CODE_DATA\\Scientific_Research\\Tangram-master\\tangram\\mapping_utils.py:8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mscanpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01msc\u001B[39;00m\n\u001B[1;32m----> 8\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlogging\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msparse\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcsc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m csc_matrix\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\site-packages\\torch\\__init__.py:81\u001B[0m\n\u001B[0;32m     77\u001B[0m     sys\u001B[38;5;241m.\u001B[39msetdlopenflags(_dl_flags\u001B[38;5;241m.\u001B[39mRTLD_GLOBAL \u001B[38;5;241m|\u001B[39m _dl_flags\u001B[38;5;241m.\u001B[39mRTLD_LAZY)\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m _dl_flags\n\u001B[1;32m---> 81\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_C\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m     83\u001B[0m __all__ \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [name \u001B[38;5;28;01mfor\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mdir\u001B[39m(_C)\n\u001B[0;32m     84\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m name[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m     85\u001B[0m             \u001B[38;5;129;01mnot\u001B[39;00m name\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBase\u001B[39m\u001B[38;5;124m'\u001B[39m)]\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m platform\u001B[38;5;241m.\u001B[39msystem() \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWindows\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing _C: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "import scipy as sp\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from anndata import AnnData\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import skimage\n",
    "import seaborn as sns\n",
    "import tangram as tg\n",
    "\n",
    "sc.logging.print_header()\n",
    "print(f\"squidpy=={sq.__version__}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets\n",
    "Load public data available in Squidpy, from mouse brain cortex. Single cell data are stored in `adata_sc`. Spatial data, in `adata_st`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T08:34:33.370211100Z",
     "start_time": "2023-09-09T08:23:03.221394400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65.5M/65.5M [08:19<00:00, 138kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Failed to open the url with default certificates, trying with certifi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 23.2M/303M [02:41<32:33, 150kB/s]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      1\u001B[0m adata_st \u001B[38;5;241m=\u001B[39m sq\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mvisium_fluo_adata_crop()\n\u001B[0;32m      2\u001B[0m adata_st \u001B[38;5;241m=\u001B[39m adata_st[\n\u001B[0;32m      3\u001B[0m     adata_st\u001B[38;5;241m.\u001B[39mobs\u001B[38;5;241m.\u001B[39mcluster\u001B[38;5;241m.\u001B[39misin([\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCortex_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mi\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39marange(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m5\u001B[39m)])\n\u001B[0;32m      4\u001B[0m ]\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m----> 5\u001B[0m img \u001B[38;5;241m=\u001B[39m \u001B[43msq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdatasets\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvisium_fluo_image_crop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m adata_sc \u001B[38;5;241m=\u001B[39m sq\u001B[38;5;241m.\u001B[39mdatasets\u001B[38;5;241m.\u001B[39msc_mouse_cortex()\n",
      "File \u001B[1;32m<string>:18\u001B[0m, in \u001B[0;36mvisium_fluo_image_crop\u001B[1;34m(path, **kwargs)\u001B[0m\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\site-packages\\squidpy\\datasets\\_utils.py:87\u001B[0m, in \u001B[0;36mMetadata.download\u001B[1;34m(self, fpath, **kwargs)\u001B[0m\n\u001B[0;32m     84\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m     85\u001B[0m     logg\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to create directory `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`. Reason `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 87\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfpath\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackup_url\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m data\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape:\n\u001B[0;32m     90\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected the data to have shape `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`, found `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\site-packages\\squidpy\\datasets\\_utils.py:167\u001B[0m, in \u001B[0;36mImgMetadata._download\u001B[1;34m(self, fpath, backup_url, **kwargs)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download\u001B[39m(\u001B[38;5;28mself\u001B[39m, fpath: PathLike, backup_url: \u001B[38;5;28mstr\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msquidpy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mim\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ImageContainer  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m--> 167\u001B[0m     \u001B[43mcheck_presence_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mPath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfpath\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbackup_url\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    169\u001B[0m     img \u001B[38;5;241m=\u001B[39m ImageContainer()\n\u001B[0;32m    170\u001B[0m     img\u001B[38;5;241m.\u001B[39madd_img(fpath, layer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m, library_id\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlibrary_id, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\site-packages\\scanpy\\_utils\\__init__.py:644\u001B[0m, in \u001B[0;36mcheck_presence_download\u001B[1;34m(filename, backup_url)\u001B[0m\n\u001B[0;32m    641\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m filename\u001B[38;5;241m.\u001B[39mis_file():\n\u001B[0;32m    642\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreadwrite\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _download\n\u001B[1;32m--> 644\u001B[0m     \u001B[43m_download\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackup_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\site-packages\\scanpy\\readwrite.py:977\u001B[0m, in \u001B[0;36m_download\u001B[1;34m(url, path)\u001B[0m\n\u001B[0;32m    975\u001B[0m                 blocknum \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    976\u001B[0m                 t\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;28mlen\u001B[39m(block))\n\u001B[1;32m--> 977\u001B[0m                 block \u001B[38;5;241m=\u001B[39m \u001B[43mresp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mblocksize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m, \u001B[38;5;167;01mException\u001B[39;00m):\n\u001B[0;32m    980\u001B[0m     \u001B[38;5;66;03m# Make sure file doesn’t exist half-downloaded\u001B[39;00m\n\u001B[0;32m    981\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m path\u001B[38;5;241m.\u001B[39mis_file():\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\http\\client.py:459\u001B[0m, in \u001B[0;36mHTTPResponse.read\u001B[1;34m(self, amt)\u001B[0m\n\u001B[0;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m amt \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    457\u001B[0m     \u001B[38;5;66;03m# Amount is given, implement using readinto\u001B[39;00m\n\u001B[0;32m    458\u001B[0m     b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mbytearray\u001B[39m(amt)\n\u001B[1;32m--> 459\u001B[0m     n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mmemoryview\u001B[39m(b)[:n]\u001B[38;5;241m.\u001B[39mtobytes()\n\u001B[0;32m    461\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    462\u001B[0m     \u001B[38;5;66;03m# Amount is not given (unbounded read) so we must check self.length\u001B[39;00m\n\u001B[0;32m    463\u001B[0m     \u001B[38;5;66;03m# and self.chunked\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\http\\client.py:503\u001B[0m, in \u001B[0;36mHTTPResponse.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    498\u001B[0m         b \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmemoryview\u001B[39m(b)[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlength]\n\u001B[0;32m    500\u001B[0m \u001B[38;5;66;03m# we do not use _safe_read() here because this may be a .will_close\u001B[39;00m\n\u001B[0;32m    501\u001B[0m \u001B[38;5;66;03m# connection, and the user is reading more bytes than will be provided\u001B[39;00m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;66;03m# (for example, reading in 1k chunks)\u001B[39;00m\n\u001B[1;32m--> 503\u001B[0m n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadinto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m n \u001B[38;5;129;01mand\u001B[39;00m b:\n\u001B[0;32m    505\u001B[0m     \u001B[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001B[39;00m\n\u001B[0;32m    506\u001B[0m     \u001B[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001B[39;00m\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_conn()\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[1;34m(self, b)\u001B[0m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[0;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\ssl.py:1241\u001B[0m, in \u001B[0;36mSSLSocket.recv_into\u001B[1;34m(self, buffer, nbytes, flags)\u001B[0m\n\u001B[0;32m   1237\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m flags \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1238\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1239\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m\n\u001B[0;32m   1240\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m-> 1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1242\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1243\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001B[1;32mD:\\Code_Studio\\My_env\\envs\\Pytorch\\lib\\ssl.py:1099\u001B[0m, in \u001B[0;36mSSLSocket.read\u001B[1;34m(self, len, buffer)\u001B[0m\n\u001B[0;32m   1097\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1098\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1099\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sslobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sslobj\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "adata_st = sq.datasets.visium_fluo_adata_crop()\n",
    "adata_st = adata_st[\n",
    "    adata_st.obs.cluster.isin([f\"Cortex_{i}\" for i in np.arange(1, 5)])\n",
    "].copy()\n",
    "img = sq.datasets.visium_fluo_image_crop()\n",
    "\n",
    "adata_sc = sq.datasets.sc_mouse_cortex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subset the crop of the mouse brain to only contain clusters of the brain cortex.  The pre-processed single cell dataset was taken from <cite data-cite=\"tasic2018shared\">Tasic et al. (2018)</cite> and pre-processed with standard scanpy functions. \n",
    "\n",
    "Let's visualize both spatial and single-cell datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_st.obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "sc.pl.spatial(\n",
    "    adata_st, color=\"cluster\", alpha=0.7, frameon=False, show=False, ax=axs[0]\n",
    ")\n",
    "sc.pl.umap(\n",
    "    adata_sc, color=\"cell_subclass\", size=10, frameon=False, show=False, ax=axs[1]\n",
    ")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "### The _Tangram_ trick: scRNA-seq are the new spatial data\n",
    "\n",
    "_Tangram_ learns a spatial alignment of the single cell data by looking at a subset of genes, specified by the user, called the training genes. Training genes need to bear interesting signal and to be measured with high quality. Typically, we choose the training genes are 100-1000 differentially expressedx genes, stratified across cell types. Sometimes, we also use the entire transcriptome, or perform different mappings using different set of training genes to see how much the result change.\n",
    "\n",
    "_Tangram_ fits the scRNA-seq profiles on space using a custom loss function based on cosine similarity. The method is summarized in the sketch below:\n",
    "\n",
    "![title](figures/how_tangram_works.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this case, we use 1401 marker genes as training genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.rank_genes_groups(adata_sc, groupby=\"cell_subclass\", use_raw=False)\n",
    "markers_df = pd.DataFrame(adata_sc.uns[\"rank_genes_groups\"][\"names\"]).iloc[0:100, :]\n",
    "markers = list(np.unique(markers_df.melt().value.values))\n",
    "len(markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepares the data using `pp_adatas`, which does the following:\n",
    "- Takes a list of genes from user via the `genes` argument. These genes are used as training genes.\n",
    "- Annotates training genes under the `training_genes` field, in `uns` dictionary, of each AnnData. \n",
    "- Ensure consistent gene order in the datasets (_Tangram_ requires that the the $j$-th column in each matrix correspond to the same gene).\n",
    "- If the counts for a gene are all zeros in one of the datasets, the gene is removed from the training genes.\n",
    "- If a gene is not present in both datasets, the gene is removed from the training genes.\n",
    "- In the pp_adatas function, the gene names are converted to lower case to get rid of the inconsistent capitalization. If this is not wanted, you can set the parameter gene_to_lowercase = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.pp_adatas(adata_sc, adata_st, genes=markers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two datasets contain 1280 training genes of the 1401 originally provided, as some training genes have been removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal spatial alignment for scRNA-seq profiles, we use the `map_cells_to_space` function:\n",
    "- The function maps iteratively as specified by `num_epochs`. We typically interrupt mapping  after the score plateaus. \n",
    "- The score measures the similarity between the gene expression of the mapped cells vs spatial data on the training genes.\n",
    "- The default mapping mode is `mode='cells'`, which is recommended to run on a GPU. \n",
    "- Alternatively, one can specify `mode='clusters'` which averages the single cells beloning to the same cluster (pass annotations via  `cluster_label`). This is faster, and is our chioce when scRNAseq and spatial data come from different specimens.\n",
    "- If you wish to run Tangram with a GPU, set `device=cuda:0` otherwise use the set `device=cpu`. \n",
    "- `density_prior` specifies the cell density within each spatial voxel. Use `uniform` if the spatial voxels are at single cell resolution (_ie_ MERFISH). The default value, `rna_count_based`, assumes that cell density is proportional to the number of RNA molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_map = tg.map_cells_to_space(adata_sc, adata_st,\n",
    "    mode=\"cells\",\n",
    "#     mode=\"clusters\",\n",
    "#     cluster_label='cell_subclass',  # .obs field w cell types\n",
    "    density_prior='rna_count_based',\n",
    "    num_epochs=500,\n",
    "    # device=\"cuda:0\",\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping results are stored in the returned `AnnData` structure, saved as `ad_map`, structured as following:\n",
    "- The cell-by-spot matrix `X` contains the probability of cell `i` to be in spot `j`.\n",
    "- The `obs` dataframe contains the metadata of the single cells.\n",
    "- The `var` dataframe contains the metadata of the spatial data.\n",
    "- The `uns` dictionary contains a dataframe with various information about the training genes (saved as `train_genes_df`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell type maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize cell types in space, we invoke `project_cell_annotation` to transfer the `annotation` from the mapping to space. We can then call `plot_cell_annotation` to visualize it. You can set the `perc` argument to set the range to the colormap, which would help remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.project_cell_annotations(ad_map, adata_st, annotation=\"cell_subclass\")\n",
    "annotation_list = list(pd.unique(adata_sc.obs['cell_subclass']))\n",
    "tg.plot_cell_annotation_sc(adata_st, annotation_list,perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way to get a sense if mapping was successful is to look for known cell type patterns. To get a deeper sense, we can use the helper `plot_training_scores` which gives us four panels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.plot_training_scores(ad_map, bins=20, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The first panel is a histogram of the simlarity scores for each training gene.\n",
    "- In the second panel, each dot is a training gene and we can observe the training score (y-axis) and the sparsity in the scRNA-seq data (x-axis) of each gene. \n",
    "- The third panel is similar to the second one, but contains the gene sparsity of the spatial data. Spatial data are usually more sparse than single cell data, a discrepancy which is often responsible for low quality mapping.\n",
    "- In the last panel, we show the training scores as a function of the difference in sparsity between the dataset. For genes with comparable sparsity, the mapped gene expression is very similar to that in the spatial data. However, if a gene is quite sparse in one dataset (typically, the spatial data) but not in other, the mapping score is lower. This occurs as Tangram cannot properly matched the gene pattern because of inconsistent amount of dropouts between the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the above plots give us a summary of scores at single-gene level, we would need to know _which_ are the genes are mapped with low scores. These information are stored in the dataframe `.uns['train_genes_df']`; this is the dataframe used to build the four plots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_map.uns['train_genes_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New spatial data via aligned single cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mapping mode is `'cells'`, we can now generate the \"new spatial data\" using the mapped single cell: this is done via `project_genes`. The function accepts as input a mapping (`adata_map`) and corresponding single cell data (`adata_sc`). The result is a voxel-by-gene `AnnData`, formally similar to `adata_st`, but containing gene expression from the mapped single cell data rather than Visium. For downstream analysis, we always replace `adata_st` with the corresponding `ad_ge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_ge = tg.project_genes(adata_map=ad_map, adata_sc=adata_sc)\n",
    "ad_ge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose a few training genes mapped with low score, to try to understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = ['rragb', 'trim17', 'eno1b']\n",
    "ad_map.uns['train_genes_df'].loc[genes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize gene patterns, we use the helper `plot_genes`. This function accepts two voxel-by-gene `AnnData`: the actual spatial data  (`adata_measured`), and a Tangram spatial prediction (`adata_predicted`). The function returns gene expression maps from the two spatial `AnnData` on the genes `genes`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.plot_genes_sc(genes, adata_measured=adata_st, adata_predicted=ad_ge, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above pictures explain the low training scores. Some genes are detected with very different levels of sparsity - typically they are much more sparse in the spatial data than in the scRNAseq. This is due to the fact that technologies like Visium are more prone to technical dropouts. Therefore, _Tangram_ cannot find a good spatial alignment for these genes as the baseline signal is missing. However, so long as _most_ training genes are measured with high quality, we can trust mapping and use _Tangram_ prediction to correct gene expression. This is an imputation method which relies on entirely different premises than those in probabilistic models. \n",
    "\n",
    "Another application is found by inspecting genes that are not detected in the spatial data, but are detected in the single cell data. They are removed before training with `pp_adatas` function, but _Tangram_ can predict their expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes=['loc102633833', 'gm5700', 'gm8292']\n",
    "tg.plot_genes_sc(genes, adata_measured=adata_st, adata_predicted=ad_ge, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So far, we only inspected genes used to align the data (training genes), but the mapped single cell data, `ad_ge` contains the whole transcriptome. That includes more than 35k test genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ad_ge.var.is_training == False).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `plot_genes` to inspect gene expression of test genes as well. Inspecting the test transcriptome is an essential to validate mapping. At the same time, we need to be careful that some prediction might disagree with spatial data because of the technical droputs.\n",
    "\n",
    "It is convenient to compute the similarity scores of all genes, which can be done by `compare_spatial_geneexp`. This function accepts two spatial AnnDatas (ie voxel-by-gene), and returns a dataframe with simlarity scores for all genes. Training genes are flagged by the boolean field `is_training`. If we also pass single cell AnnData to `compare_spatial_geneexp` function like below, a dataframe with additional sparsity columns - sparsity_sc (single cell data sparsity) and sparsity_diff (spatial data sparsity - single cell data sparsity) will return. This is required if we want to call `plot_test_scores` function later with the returned datafrme from `compare_spatial_geneexp` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_genes = tg.compare_spatial_geneexp(ad_ge, adata_st, adata_sc)\n",
    "df_all_genes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction on test genes can be graphically visualized using `plot_auc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(data=df_all_genes, x='score', y='sparsity_sp', hue='is_training', alpha=.5);  # for legacy\n",
    "tg.plot_auc(df_all_genes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This above figure is the most important validation plot in _Tangram_.** Each dot represents a gene; the x-axis indicates the score, and the y-axis the sparsity of that gene in the spatial data.  Unsurprisingly, the genes predicted with low score represents very sparse genes in the spatial data, suggesting that the _Tangram_ predictions correct expression in those genes. Note that curve observed above is typical of _Tangram_ mappings: the area under that curve is the most reliable metric we use to evaluate mapping.\n",
    "\n",
    "Let's inspect a few predictions. Some of these genes are biologically sparse, but well predicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes=['tfap2b', 'zic4']\n",
    "tg.plot_genes_sc(genes, adata_measured=adata_st, adata_predicted=ad_ge, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some non-sparse genes present petterns, that _Tangram_ accentuates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = ['cd34', 'rasal1']\n",
    "tg.plot_genes_sc(genes, adata_measured=adata_st, adata_predicted=ad_ge, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some unannotated genes have unknown function. These genes are often hardly detected in spatial data but _Tangram_ provides prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes = ['gm33027', 'gm5431']\n",
    "tg.plot_genes_sc(genes[:5], adata_measured=adata_st, adata_predicted=ad_ge, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvolution\n",
    "\n",
    "For untargeted spatial technologies, like Visium and Slide-seq, a spatial voxel may contain more than one cells. In these cases, it might be useful to disentangle gene expression into single cells - a process called deconvolution. Deconvolution is a requested feature, and also hard to obtain accurately with computational methods. If your goal is to study co-localization of cell types, we recommend you work with the spatial cell type maps instead. If your aim is discovery of cell-cell communication mechanisms, we suggest you compute gene programs, then use `project_cell_annotations` to spatially visualize program usage. To proceed with deconvolution anyways, see below.\n",
    "\n",
    "In order to deconvolve cells, _Tangram_ needs to know how many cells are present in each voxel. This is achieved by segmenting the cells on the corresponding histology, which squidpy makes possible with two lines of code:\n",
    "- `squidpy.im.process` applies smoothing as a pre-processing step.\n",
    "- `squidpy.im.segment` computes segmentation masks with watershed algorithm.\n",
    "\n",
    "Note that some technologies, like Slide-seq, currently do not allow staining the same slide of tissue on which genes are profiled. For these data, you can still attempt a deconvolution by estimating cell density in a rough way - often we just pass a uniform prior. Finally, note that deconvolutions are hard to validate, as we do not have ground truth spatially-resolved single cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq.im.process(img=img, layer=\"image\", method=\"smooth\")\n",
    "sq.im.segment(\n",
    "    img=img,\n",
    "    layer=\"image_smooth\",\n",
    "    method=\"watershed\",\n",
    "    channel=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the segmentation results for an inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inset_y = 1500\n",
    "inset_x = 1700\n",
    "inset_sy = 400\n",
    "inset_sx = 500\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 10))\n",
    "sc.pl.spatial(\n",
    "    adata_st, color=\"cluster\", alpha=0.7, frameon=False, show=False, ax=axs[0], title=\"\"\n",
    ")\n",
    "axs[0].set_title(\"Clusters\", fontdict={\"fontsize\": 20})\n",
    "sf = adata_st.uns[\"spatial\"][\"V1_Adult_Mouse_Brain_Coronal_Section_2\"][\"scalefactors\"][\n",
    "    \"tissue_hires_scalef\"\n",
    "]\n",
    "rect = mpl.patches.Rectangle(\n",
    "    (inset_y * sf, inset_x * sf),\n",
    "    width=inset_sx * sf,\n",
    "    height=inset_sy * sf,\n",
    "    ec=\"yellow\",\n",
    "    lw=4,\n",
    "    fill=False,\n",
    ")\n",
    "axs[0].add_patch(rect)\n",
    "\n",
    "axs[0].axes.xaxis.label.set_visible(False)\n",
    "axs[0].axes.yaxis.label.set_visible(False)\n",
    "\n",
    "axs[1].imshow(\n",
    "    img[\"image\"][inset_y : inset_y + inset_sy, inset_x : inset_x + inset_sx, 0, 0]\n",
    "    / 65536,\n",
    "    interpolation=\"none\",\n",
    ")\n",
    "axs[1].grid(False)\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_title(\"DAPI\", fontdict={\"fontsize\": 20})\n",
    "\n",
    "crop = img[\"segmented_watershed\"][\n",
    "    inset_y : inset_y + inset_sy, inset_x : inset_x + inset_sx\n",
    "].values.squeeze(-1)\n",
    "crop = skimage.segmentation.relabel_sequential(crop)[0]\n",
    "cmap = plt.cm.plasma\n",
    "cmap.set_under(color=\"black\")\n",
    "axs[2].imshow(crop, interpolation=\"none\", cmap=cmap, vmin=0.001)\n",
    "axs[2].grid(False)\n",
    "axs[2].set_xticks([])\n",
    "axs[2].set_yticks([])\n",
    "axs[2].set_title(\"Nucleous segmentation\", fontdict={\"fontsize\": 20});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between DAPI and mask confirms the quality of the segmentation. We then need to extract some image features useful for the deconvolution task downstream. Specifically:\n",
    "- the number of unique segmentation objects (i.e. nuclei) under each spot.\n",
    "- the coordinates of the centroids of the segmentation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image layer to use for segmentation\n",
    "features_kwargs = {\n",
    "    \"segmentation\": {\n",
    "        \"label_layer\": \"segmented_watershed\",\n",
    "        \"props\": [\"label\", \"centroid\"],\n",
    "        \"channels\": [1, 2],\n",
    "    }\n",
    "}\n",
    "# calculate segmentation features\n",
    "sq.im.calculate_image_features(\n",
    "    adata_st,\n",
    "    img,\n",
    "    layer=\"image\",\n",
    "    key_added=\"image_features\",\n",
    "    features_kwargs=features_kwargs,\n",
    "    features=\"segmentation\",\n",
    "    mask_circle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the total number of objects under each spot with scanpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_st.obs[\"cell_count\"] = adata_st.obsm[\"image_features\"][\"segmentation_label\"]\n",
    "sc.pl.spatial(adata_st, color=[\"cluster\", \"cell_count\"], frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconvolution via alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rationale for deconvolving with Tangram, is to constrain the number of mapped single cell profiles. This is different that most deconvolution method. Specifically, we set them equal to the number of segmented cells in the histology, in the following way:\n",
    "- We pass `mode='constrained'`. This adds a filter term to the loss function, and a boolean regularizer.\n",
    "- We set `target_count` equal to the total number of segmented cells. _Tangram_ will look for the best `target_count` cells to align in space.\n",
    "- We pass a `density_prior`, containing the fraction of cells per voxel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_map = tg.map_cells_to_space(\n",
    "    adata_sc,\n",
    "    adata_st,\n",
    "    mode=\"constrained\",\n",
    "    target_count=adata_st.obs.cell_count.sum(),\n",
    "    density_prior=np.array(adata_st.obs.cell_count) / adata_st.obs.cell_count.sum(),\n",
    "    num_epochs=1000,\n",
    "#     device=\"cuda:0\",\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way as before, we can plot cell type maps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.project_cell_annotations(ad_map, adata_st, annotation=\"cell_subclass\")\n",
    "annotation_list = list(pd.unique(adata_sc.obs['cell_subclass']))\n",
    "tg.plot_cell_annotation_sc(adata_st, annotation_list, perc=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate mapping by inspecting the test transcriptome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_ge = tg.project_genes(adata_map=ad_map, adata_sc=adata_sc)\n",
    "df_all_genes = tg.compare_spatial_geneexp(ad_ge, adata_st, adata_sc)\n",
    "tg.plot_auc(df_all_genes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here comes the key part, where we will use the results of the previous deconvolution steps. Previously, we computed the absolute numbers of unique segmentation objects under each spot, together with their centroids. Let's extract them in the right format useful for _Tangram_. In the resulting dataframe, each row represents a single segmentation object (a cell). We also have the image coordinates as well as the unique centroid ID, which is a string that contains both the spot ID and a numerical index. _Tangram_ provides a convenient function to export the mapping between spot ID and segmentation ID to `adata.uns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.create_segment_cell_df(adata_st)\n",
    "adata_st.uns[\"tangram_cell_segmentation\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `tangram.count_cell_annotation()` to map cell types as result of the deconvolution step to putative segmentation ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tg.count_cell_annotations(\n",
    "    ad_map,\n",
    "    adata_sc,\n",
    "    adata_st,\n",
    "    annotation=\"cell_subclass\",\n",
    ")\n",
    "adata_st.obsm[\"tangram_ct_count\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally export the results in a new `AnnData` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_segment = tg.deconvolve_cell_annotations(adata_st)\n",
    "adata_segment.obs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the AnnData object does not contain counts, but only cell type annotations, as results of the Tangram mapping.  Nevertheless, it's convenient to create such AnnData object for visualization purposes. Below you can appreciate how each dot is now not a Visium spot anymore, but a single unique segmentation object, with the mapped cell type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbsphinx-thumbnail": {
     "tooltip": "Use Tangram for cell-type deconvolution."
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(20, 20))\n",
    "sc.pl.spatial(\n",
    "    adata_segment,\n",
    "    color=\"cluster\",\n",
    "    size=0.4,\n",
    "    show=False,\n",
    "    frameon=False,\n",
    "    alpha_img=0.2,\n",
    "    legend_fontsize=20,\n",
    "    ax=ax,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6a69e99b54b137c0a46b7eb1c01877b00eaeac81f3c7d96786977ff3fe6ebe1"
  },
  "kernelspec": {
   "name": "conda-env-Pytorch-py",
   "language": "python",
   "display_name": "Python [conda env:Pytorch] *"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
